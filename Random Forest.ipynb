{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Report - Kyle Lombardo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the recent uptick in hospital readmissions due to the addition of COPD and Hip & Knee measurement and historical trends, there is a need to understand why there has been an historically high number of readmissions in the hospital system. This executive report builds off the previous reports and will attempt to answer the queston of whether there are certain demographics, locales, pre-existing conditions, or other variables that will result in a higher likelihood of readmission.\n",
    "\n",
    "In the Exploratory Data Analysis report showed that there was a statistical relationship between 'Population', 'Children', 'Initial_days', and 'TotalCharge' with 'ReAdmis' when compared using a t-test. These results were similar to the first report which found that there was a relationship between 'ReAdmis', 'TotalCharge', 'Initial_days', and 'VitD_levels' when analyzed using PCA.\n",
    "\n",
    "In a previous report using logistic regression, there was a very strong logistic relationship between 'ReAdmis' with 'Item8' and 'Initial_days' with an accuracy of 98%, auc of roc of 0.998, and a confusion matrix of:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th colspan='2'>Logistic Regression</th>\n",
    "        </tr>\n",
    "    </thead>    \n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td><strong>Predict ReAdmis</strong></td>\n",
    "        <td><strong>Predict Not</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual ReAdmis</td>\n",
    "        <td>1241</td>\n",
    "        <td>25</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Not</td>\n",
    "        <td>25</td>\n",
    "        <td>709</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "In another report, K-nearest neighbor proved to be less effective than logistic regression even with hyperparameter tuning for `n_neighbors`. KNN gave an accuracy score of 95%, auc of roc of 0.952, and a confusion matrix of:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th colspan='2'>K-nearest neighbor</th>\n",
    "        </tr>\n",
    "    </thead>    \n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td><strong>Predict ReAdmis</strong></td>\n",
    "        <td><strong>Predict Not</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual ReAdmis</td>\n",
    "        <td>1232</td>\n",
    "        <td>47</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Not</td>\n",
    "        <td>43</td>\n",
    "        <td>678</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "This report will use Random Forest, a supervised machine learning |algorithm, to see if it is a more accurate method than logistic regression and KNN in predicting whether a patient will be readmitted into the hospital after being discharged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this report includes 10,000 patient record's responses to the following criteria from 'D207 D208 D209 Medical Data Consideration and Dictionary.pdf'. All strings that could not be re-expressed were removed as they would be impossible to use for random forest. Redundant data such as 'Lat' and 'Lng' compared to 'Zip' or meaningless data such as 'Customer_id' were removed. Lastly, because random forest is only compatible with continuous values, many features will need to be re-expressed as well.\n",
    "\n",
    "(Criteria marked with a * were removed for this report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria | Data Type | Example | Description\n",
    "----- | ----- | ----- | -----\n",
    "*Case Order | Integer | 1034 | Index Values\n",
    "*Customer_id\t|String |‘D550524’|\tUnique identifier for patient\n",
    "*Interaction |\tString|\t‘8cd49b13-f45a-4b47-a2bd-173ffa932c2f’|\tUnique Identifier for transaction, procedure, and admission\n",
    "*UID\t|String|\t‘3a83ddb66e2ae73798bdf1d705dc0932’\t|Unique Identifier for transaction, procedure, and admission\n",
    "*City\t|String\t|‘Braggs’\t|Patient city address from billing\n",
    "*State\t|String|\t‘AL’|\tPatient state address from billing\n",
    "*County\t|String|\t‘Morgan’|\tPatient county address from billing\n",
    "Zip\t|Integer|\t35621\t|Patient zip address from billing\n",
    "*Lat\t|Float\t|-86.5404\t|Patient latitude address from billing\n",
    "*Lng\t|Float\t|-81.1272\t|Patient longitude address from billing\n",
    "Population\t|Integer|\t281\t|Patient city address population from billing\n",
    "Area\t|String\t|‘Urban’|\tPatient address from billing zoning type\n",
    "*Timezone|\tString|\t‘America/New_York’|\tPatient address from billing time zone locale\n",
    "*Job\t|String\t|‘Actuary’\t|Patient or primary insurance holder’s occupation \n",
    "Children|\tInteger|\t2|\tNumber of children in patient’s household\n",
    "Age\t|Integer|\t53\t|Patient’s age\n",
    "Income|\tFloat|\t88126.93|\tPatient or primary insurance holder’s yearly income\n",
    "Marital|\tString|\t‘Married’|\tPatient’s marital status\n",
    "Gender\t|String|\t‘Female’\t|Patient’s self-identification of gender\n",
    "ReAdmis\t|String\t|‘Yes’\t|Whether patient has been readmitted within a month of last release\n",
    "VitD_level|\tFloat|\t47.81348|\tPatient’s vitamin d level (ng/mL)\n",
    "Doc_visits|\tInteger|\t5\t|Number of visits by primary physician during initial hospitalization\n",
    "Full_meals_eaten|\tInteger\t|1|\tNumber of full meals the patient ate while in hospital.\n",
    "VitD_supp|\tInteger|\t2\t|Number of times vitamin d supplement was ministered to patient\n",
    "Soft_drink\t|String|\t‘Yes’|\tWhether patient consumes >= 3 soft drinks in a day\n",
    "Initial_admin\t|String\t|‘Observation Admission’\t|Means by which patient was admitted to hospital initially\n",
    "HighBlood\t|String\t|‘Yes’\t|Whether patient has high blood pressure\n",
    "Stroke\t|String|\t‘No’\t|Whether patient has had a stroke\n",
    "Complication_risk|\tString\t|‘High’\t|Complication risk level as determined by primary physician\n",
    "Overweight|\tString\t|‘Yes’\t|Whether patient is considered obese considering his or her demographics\n",
    "Arthritis|\tString\t|‘No’|\tWhether patient has arthritis\n",
    "Diabetes\t|String\t|‘Yes’|\tWhether patient has diabetes\n",
    "Hyperlipidemia\t|String|\t‘No’|\tWhether patient has hyperlipidemia\n",
    "BackPain\t|String\t|‘Yes’|\tWhether patient has chronic back pain\n",
    "Anxiety\t|String\t|‘Yes’\t|Whether patient has anxiety\n",
    "Allergic_rhinitis\t|String\t|‘No’|\tWhether patient has allergic rhinitis\n",
    "Reflux_esophagitis\t|String\t|‘Yes’|\tWhether patient has reflux esophagitis\n",
    "Asthma\t|String|\t‘No’|\tWhether patient has asthma\n",
    "Services\t|String|\t‘CT Scan’\t|Primary service patient received from hospital\n",
    "Initial_days|\tFloat\t|7.302395\t|Number of days patient stayed in hospital on initial visit\n",
    "TotalCharge\t|Float\t|2631.702|\tAverage amount charged to patient daily \n",
    "Additional_charges|\tFloat|\t14382.23|\tAverage amount for miscellaneous procedures, treatments, medicines, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patient’s opinion survey on rate of importance (1 = most important, 8 = least important)\n",
    "\n",
    "Criteria | Data Type | Example | Description\n",
    "----- | ----- | ----- | -----\n",
    "Item1 |Integer| 3| Timely admission\n",
    "Item2\t|Integer|2|\tTimely treatment\n",
    "Item3\t|Integer|5|\tTimely visits\n",
    "Item4\t|Integer|6|\tReliability\n",
    "Item5\t|Integer|1|\tOptions\n",
    "Item6\t|Integer|2|\tHours of treatment\n",
    "Item7\t|Integer|6|\tCourteous staff\n",
    "Item8\t|Integer|4|\tEvidence of active listening from doctor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Method Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a useful machine learning tool used to predict either categorical and regression target variables using multiple continuous variables in a data set. Random forest is made of a user defined number of decision trees each with different random entries of the same size. It accomplishes this by randomly selecting a fixed number of entries, in this case 10,000, with replacement meaning that the same entry can be used multiple times. In addition, random forest also picks features from a random subset for each decision tree. Both of these attributes of random forest ensures that the method for the decision making is different for each unique decision tree.<sup>1</sup>\n",
    "\n",
    "When it comes time for a prediction, random forest will send the entry in question through each decision tree, tally up all the votes from each decision tree, and the highest vote will be the final prediction. In this particular case, it will predict those patients that are or are not readmitted.\n",
    "\n",
    "Some assumptions for random forest to work properly is that all data fed into the model as predictors must be cleaned with no missing data. All feature variables must be continuous data so no categorical data in the form of strings can be used in the data set. All categorical data will be converted using one-hot encoding through the python method `pd.get_dummies()`. This will result in a much higher number of features for the final data set as each categorical variable will be split into n number of features where n = number of unique categories - 1.<sup>2</sup>\n",
    "\n",
    "In this report Python will be used. While R could be used, the python scikit learn library has many useful tools including hyperparameter tuning with gridsearch, pipelines, and modeling techniques that makes random forest easy and quick to use. \n",
    "\n",
    "As for libraries used in the report, the following will be utilized. Pandas and numpy are crucial tools for importing, re-expressing, and manipulating the data. Scikit Learn is used for multiple things in this report including:\n",
    "\n",
    "<ol>\n",
    "    <li><strong><em>Pipeline</em></strong>: a way to group many scikit learn methods together in a clean and quick fashion. </li>\n",
    "    <li><strong><em>GridSearchCV</em></strong>: used for hyperparameter tuning. It will try a number of different combinations of hyperparameters in a given range and output the best one according to a score. This score will be set to accuracy for random forest.</li>\n",
    "    <li><strong><em>train_test_split</em></strong>: used in order to split up the feature and target variables into separate training and testing set. This is best practice in order to test the model using outside information, or information not used in the training set.</li>\n",
    "    <li><strong><em>RandomForestClassifier</em></strong>: random forest model method already described above.</li>\n",
    "    <li><strong><em>classification_report</em></strong>: gives a summary of accuracy, recall, and precision among other parameters.</li>\n",
    "    <li><strong><em>confusion_matrix</em></strong>: a tabled statistical summary of true positives, true negatives, false positives, and false negatives of the prediction values vs the actual values</li>\n",
    "    <li><strong><em>roc_auc_score</em></strong>: area under the curve of the graph plotting the true positive rate vs. the false positive rate also known as receiver operating curve. A perfect score includes only true positive values and gives an area of 1.0</li>\n",
    "</ol>\n",
    "\n",
    "The data set 'df' is imported here using pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "raw_data = pd.read_csv('medical_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the data must be in the correct format and data types in order to be run properly in random forest. The data set will need to undergo a data cleaning step as follows: \n",
    "\n",
    "First, all variables not being used will be removed from the data set. Only numeric variables or variables that can be re-expressed as numeric will remain. This is the only data type random forest is able to use. Next, re-expressions will be performed using one-hot encoding using the `get_dummies()`, a pandas method, on all categorical variables including the target variable. This will break up all categorical variables into separate binary variables. The first binary variable is dropped from each previous variable due to being redundant. \n",
    "\n",
    "Second, any missing values must be imputed using either an average or the most frequent value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnecessary and redundant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.drop(['CaseOrder', 'Customer_id', 'Interaction', 'UID', 'City', 'State', \\\n",
    "                    'County', 'Lat', 'Lng', 'TimeZone','Job'], axis=1)\n",
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-expressing data to numeric data types and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick way to see how many missing values are in the data set `df.info()` can be used. It looks as if there is no missing data values. Imputation will not need to be performed on this data set. Also, all data types are either integers or floats and are ready to be run using random forest.\n",
    "\n",
    "With the data set `df` cleaned and in correct type format, the mean and variance for each feature can be calculated using `df.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 48 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   Zip                                  10000 non-null  int64  \n",
      " 1   Population                           10000 non-null  int64  \n",
      " 2   Children                             10000 non-null  int64  \n",
      " 3   Age                                  10000 non-null  int64  \n",
      " 4   Income                               10000 non-null  float64\n",
      " 5   VitD_levels                          10000 non-null  float64\n",
      " 6   Doc_visits                           10000 non-null  int64  \n",
      " 7   Full_meals_eaten                     10000 non-null  int64  \n",
      " 8   vitD_supp                            10000 non-null  int64  \n",
      " 9   Initial_days                         10000 non-null  float64\n",
      " 10  TotalCharge                          10000 non-null  float64\n",
      " 11  Additional_charges                   10000 non-null  float64\n",
      " 12  Item1                                10000 non-null  int64  \n",
      " 13  Item2                                10000 non-null  int64  \n",
      " 14  Item3                                10000 non-null  int64  \n",
      " 15  Item4                                10000 non-null  int64  \n",
      " 16  Item5                                10000 non-null  int64  \n",
      " 17  Item6                                10000 non-null  int64  \n",
      " 18  Item7                                10000 non-null  int64  \n",
      " 19  Item8                                10000 non-null  int64  \n",
      " 20  Area_Suburban                        10000 non-null  uint8  \n",
      " 21  Area_Urban                           10000 non-null  uint8  \n",
      " 22  Marital_Married                      10000 non-null  uint8  \n",
      " 23  Marital_Never Married                10000 non-null  uint8  \n",
      " 24  Marital_Separated                    10000 non-null  uint8  \n",
      " 25  Marital_Widowed                      10000 non-null  uint8  \n",
      " 26  Gender_Male                          10000 non-null  uint8  \n",
      " 27  Gender_Nonbinary                     10000 non-null  uint8  \n",
      " 28  ReAdmis_Yes                          10000 non-null  uint8  \n",
      " 29  Soft_drink_Yes                       10000 non-null  uint8  \n",
      " 30  Initial_admin_Emergency Admission    10000 non-null  uint8  \n",
      " 31  Initial_admin_Observation Admission  10000 non-null  uint8  \n",
      " 32  HighBlood_Yes                        10000 non-null  uint8  \n",
      " 33  Stroke_Yes                           10000 non-null  uint8  \n",
      " 34  Complication_risk_Low                10000 non-null  uint8  \n",
      " 35  Complication_risk_Medium             10000 non-null  uint8  \n",
      " 36  Overweight_Yes                       10000 non-null  uint8  \n",
      " 37  Arthritis_Yes                        10000 non-null  uint8  \n",
      " 38  Diabetes_Yes                         10000 non-null  uint8  \n",
      " 39  Hyperlipidemia_Yes                   10000 non-null  uint8  \n",
      " 40  BackPain_Yes                         10000 non-null  uint8  \n",
      " 41  Anxiety_Yes                          10000 non-null  uint8  \n",
      " 42  Allergic_rhinitis_Yes                10000 non-null  uint8  \n",
      " 43  Reflux_esophagitis_Yes               10000 non-null  uint8  \n",
      " 44  Asthma_Yes                           10000 non-null  uint8  \n",
      " 45  Services_CT Scan                     10000 non-null  uint8  \n",
      " 46  Services_Intravenous                 10000 non-null  uint8  \n",
      " 47  Services_MRI                         10000 non-null  uint8  \n",
      "dtypes: float64(5), int64(15), uint8(28)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zip</th>\n",
       "      <th>Population</th>\n",
       "      <th>Children</th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>VitD_levels</th>\n",
       "      <th>Doc_visits</th>\n",
       "      <th>Full_meals_eaten</th>\n",
       "      <th>vitD_supp</th>\n",
       "      <th>Initial_days</th>\n",
       "      <th>...</th>\n",
       "      <th>Diabetes_Yes</th>\n",
       "      <th>Hyperlipidemia_Yes</th>\n",
       "      <th>BackPain_Yes</th>\n",
       "      <th>Anxiety_Yes</th>\n",
       "      <th>Allergic_rhinitis_Yes</th>\n",
       "      <th>Reflux_esophagitis_Yes</th>\n",
       "      <th>Asthma_Yes</th>\n",
       "      <th>Services_CT Scan</th>\n",
       "      <th>Services_Intravenous</th>\n",
       "      <th>Services_MRI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50159.323900</td>\n",
       "      <td>9965.253800</td>\n",
       "      <td>2.097200</td>\n",
       "      <td>53.511700</td>\n",
       "      <td>40490.495160</td>\n",
       "      <td>17.964262</td>\n",
       "      <td>5.012200</td>\n",
       "      <td>1.001400</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>34.455299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27380</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.28930</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27469.588208</td>\n",
       "      <td>14824.758614</td>\n",
       "      <td>2.163659</td>\n",
       "      <td>20.638538</td>\n",
       "      <td>28521.153293</td>\n",
       "      <td>2.017231</td>\n",
       "      <td>1.045734</td>\n",
       "      <td>1.008117</td>\n",
       "      <td>0.628505</td>\n",
       "      <td>26.309341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.44593</td>\n",
       "      <td>0.472777</td>\n",
       "      <td>0.492112</td>\n",
       "      <td>0.467076</td>\n",
       "      <td>0.488681</td>\n",
       "      <td>0.492486</td>\n",
       "      <td>0.45346</td>\n",
       "      <td>0.327879</td>\n",
       "      <td>0.463738</td>\n",
       "      <td>0.191206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>154.080000</td>\n",
       "      <td>9.806483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.001981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27592.000000</td>\n",
       "      <td>694.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>19598.775000</td>\n",
       "      <td>16.626439</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.896215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50207.000000</td>\n",
       "      <td>2769.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>33768.420000</td>\n",
       "      <td>17.951122</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.836244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72411.750000</td>\n",
       "      <td>13945.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>54296.402500</td>\n",
       "      <td>19.347963</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61.161020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99929.000000</td>\n",
       "      <td>122814.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>207249.100000</td>\n",
       "      <td>26.394449</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>71.981490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Zip     Population      Children           Age         Income  \\\n",
       "count  10000.000000   10000.000000  10000.000000  10000.000000   10000.000000   \n",
       "mean   50159.323900    9965.253800      2.097200     53.511700   40490.495160   \n",
       "std    27469.588208   14824.758614      2.163659     20.638538   28521.153293   \n",
       "min      610.000000       0.000000      0.000000     18.000000     154.080000   \n",
       "25%    27592.000000     694.750000      0.000000     36.000000   19598.775000   \n",
       "50%    50207.000000    2769.000000      1.000000     53.000000   33768.420000   \n",
       "75%    72411.750000   13945.000000      3.000000     71.000000   54296.402500   \n",
       "max    99929.000000  122814.000000     10.000000     89.000000  207249.100000   \n",
       "\n",
       "        VitD_levels    Doc_visits  Full_meals_eaten     vitD_supp  \\\n",
       "count  10000.000000  10000.000000      10000.000000  10000.000000   \n",
       "mean      17.964262      5.012200          1.001400      0.398900   \n",
       "std        2.017231      1.045734          1.008117      0.628505   \n",
       "min        9.806483      1.000000          0.000000      0.000000   \n",
       "25%       16.626439      4.000000          0.000000      0.000000   \n",
       "50%       17.951122      5.000000          1.000000      0.000000   \n",
       "75%       19.347963      6.000000          2.000000      1.000000   \n",
       "max       26.394449      9.000000          7.000000      5.000000   \n",
       "\n",
       "       Initial_days  ...  Diabetes_Yes  Hyperlipidemia_Yes  BackPain_Yes  \\\n",
       "count  10000.000000  ...   10000.00000        10000.000000  10000.000000   \n",
       "mean      34.455299  ...       0.27380            0.337200      0.411400   \n",
       "std       26.309341  ...       0.44593            0.472777      0.492112   \n",
       "min        1.001981  ...       0.00000            0.000000      0.000000   \n",
       "25%        7.896215  ...       0.00000            0.000000      0.000000   \n",
       "50%       35.836244  ...       0.00000            0.000000      0.000000   \n",
       "75%       61.161020  ...       1.00000            1.000000      1.000000   \n",
       "max       71.981490  ...       1.00000            1.000000      1.000000   \n",
       "\n",
       "        Anxiety_Yes  Allergic_rhinitis_Yes  Reflux_esophagitis_Yes  \\\n",
       "count  10000.000000           10000.000000            10000.000000   \n",
       "mean       0.321500               0.394100                0.413500   \n",
       "std        0.467076               0.488681                0.492486   \n",
       "min        0.000000               0.000000                0.000000   \n",
       "25%        0.000000               0.000000                0.000000   \n",
       "50%        0.000000               0.000000                0.000000   \n",
       "75%        1.000000               1.000000                1.000000   \n",
       "max        1.000000               1.000000                1.000000   \n",
       "\n",
       "        Asthma_Yes  Services_CT Scan  Services_Intravenous  Services_MRI  \n",
       "count  10000.00000      10000.000000          10000.000000  10000.000000  \n",
       "mean       0.28930          0.122500              0.313000      0.038000  \n",
       "std        0.45346          0.327879              0.463738      0.191206  \n",
       "min        0.00000          0.000000              0.000000      0.000000  \n",
       "25%        0.00000          0.000000              0.000000      0.000000  \n",
       "50%        0.00000          0.000000              0.000000      0.000000  \n",
       "75%        1.00000          0.000000              1.000000      0.000000  \n",
       "max        1.00000          1.000000              1.000000      1.000000  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the last step in the data preparation stage, the cleaned and prepared dataset is exported for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\kylel\\OneDrive\\Desktop\\Learning\\School\\D209 - Data Mining I\\PA2\\prepped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data cleaned and prepared, it is time to run the random forest model. The data will need to be split into `X` for features and `y` as the target. \n",
    "\n",
    "First, a pipeline will be set up to make things a bit more organized. This pipeline will use `RandomForestClassifier()` and allow the use of `GridSearchCV` to be run.  \n",
    "\n",
    "Next, the data will be split into training and testing sets using `train_test_split()` with 80% going to the training set and the remaining 20% to the testing set. This is a crucial step in the modeling process. If all of `X` is used, there would be no way to test the model's accuracy. Splitting the data into training and testing sets allows for the ability to use unseen data (the test set) to be used to test for accuracy.\n",
    "\n",
    "The X_train and y_train (along with the X_test and y_test) are merged together using the pandas merge function and exported to separate csvs for testing and training. \n",
    "\n",
    "After, a `GridSearchCV()` will be used to tune the `n_estimators` hyperparameter. This will cycle through a range of a number of decision trees for the `n_estimators` values that gives the highest accuracy. This was already done a few times ahead of time to find the best range to iterate through. GridSearch typically has a default scoring of R<sup>2</sup>, but the best scoring parameter for a binary target would be accuracy.\n",
    "\n",
    "Finally, the model will be fit and ready to analyze and be examined.<sup>2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['ReAdmis_Yes'], axis=1)\n",
    "y = df['ReAdmis_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('RFC', RandomForestClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.merge(X_train, pd.DataFrame(y_train), left_index=True, right_index=True)\n",
    "training_set.to_csv(r'C:\\Users\\kylel\\OneDrive\\Desktop\\Learning\\School\\D209 - Data Mining I\\PA2\\training_data.csv')\n",
    "\n",
    "testing_set = pd.merge(X_test, pd.DataFrame(y_test), left_index=True, right_index=True)\n",
    "testing_set.to_csv(r'C:\\Users\\kylel\\OneDrive\\Desktop\\Learning\\School\\D209 - Data Mining I\\PA2\\testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'RFC__n_estimators': np.arange(210,230,2)}\n",
    "cv = GridSearchCV(pipeline, parameters, cv=3, scoring='accuracy')\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1279\n",
      "           1       0.98      0.97      0.97       721\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n",
      "Tuned Model Parameters: {'RFC__n_estimators': 224}\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print('Tuned Model Parameters: {}'.format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1262,   17],\n",
       "       [  23,  698]], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confuse_matrix = confusion_matrix(y_test, y_pred)\n",
    "confuse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc score: 0.977\n"
     ]
    }
   ],
   "source": [
    "roc = roc_auc_score(y_test, y_pred)\n",
    "print('Auc score: {:.3f}'.format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "mse = MSE(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Data Summary and Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range in the `parameters` variable has been tuned a number of times to find the value that works best for this model. With the number of estimators set to 212 (although this will differ from run to run due to the randomness of random forest) providing the highest accuracy for the test set.\n",
    "\n",
    "A classification_report was also used to see what sort of accuracy this particular model gives. 98% is even with the logistic regression when using all features whether they help or hinder the prediction. When comparing confusion matrices, the random forest does perform slighty better with lower values for both false positives and false negatives. Logistic regression does have a higher `roc_auc_score` with 0.998 vs random forest's 0.977 (which can also differ slightly). This model also gives a mean squared error of 0.019 (yet again, can differ slightly) but mse is not an appropriate metric for classification and should be ignored. \n",
    "\n",
    "However, with all that has been said, random foreset is able to make a solid, quick prediction (in the sense of programming and hyper parameter tuning) of whether a patient may or may not be readmitted to the hospital. One of the biggest limitations of random forest is the speed at which the algorithms works. Creating such a high number of decision trees takes a long toll on the speed of the modeling. Another limitation is the data preparation that needs to happen before it can even be run including removing an null values and its ability to only use numeric values.<sup>3</sup>\n",
    "\n",
    "The recommended actions at this point are to either use the logitistic regression covered in a previous report or this random forest model given that they both have similar predictive outputs. The logistic model took longer to tune due to feature reduction but can perform quickly now. The random forest does take a bit longer to perform but gives similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Elie Kawerk, <em>Machine Learning with Tree-Based Models in Python</em>, DataCamp, accessed 22 October 2021, <<https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-python/bagging-and-random-forests?ex=7>>.\n",
    "2. Hugo Bowne-Anderson, <em>Supervised Learning with scikit-learn</em>, DataCamp, accessed 22 October 2021, <<https://app.datacamp.com/learn/courses/supervised-learning-with-scikit-learn>>.\n",
    "3. Niklas Donges, <em>A Complete Guide to the Random Forest Algorithm</em>, Built In Beta, accessed 22, October 2021, <<https://builtin.com/data-science/random-forest-algorithm#procon>>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
